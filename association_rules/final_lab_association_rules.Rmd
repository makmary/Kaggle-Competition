---
title: "final_lab_association_rules"
author: "andrey"
date: "08 01 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Association rules
Попробуем использовать ассоциативные правила, чтобы найти зависимости в данных.  
В данных есть бинарные и небинарные колонки, а так же первая, неподходящая колонка с временем выполнения опроса
Чтобы применить ассоциативные правила, нужно преобразовать бинарные колонки в TRUE/FALSE, небинарные(но факторные) сами преобразуются в множество бинарных, первую колонку нужно убрать. Убираю supplementary questions, мало ли кто там чего хочет освоить, пока не освоит, не поймет, что это ему не подходит.  
Подключаем нужные пакеты для ассоциативных правил
```{r, message=FALSE}
library('arules')
library ('arulesViz')
```
Подключаем пакет для использования регулярных выражений
```{r}
library(dplyr)
```
Загружаем данные, так, чтобы заголовками были названия вопросов
```{r}
data2 = read.csv("kaggle_survey_2020_responses.csv", skip = 1,na.strings="")
```
Убираем 1 столбец и supplementary questions
```{r}
data3 = data2[2:256]
```
Делаем новый датасет только из тех столбцов, в которых бинарные значения
```{r}
data_binary = is.na(select(data3, contains("Select.all.that.apply")))==FALSE
```
Делаем новый датасет из оставшихся столбцов
```{r}
data_non_binary = select(data3, !contains("Select.all.that.apply"))
```
Преобразуем датасет в факторы
```{r}
for (i in 1:length(data_non_binary))
{
  data_non_binary[[i]] = factor(data_non_binary[[i]])
}
```
Делаем один общий датасет из этих двух датасетов
```{r}
data_for_assoc = cbind(data_non_binary,data_binary)
```
Преобразуем это в обьект для использования ассоциативных правил
```{r}
data_assoc = as(data_for_assoc,"transactions")
```
Посмотрим на самые часто выбираемые варианты
```{r}
tail(sort(itemFrequency(data_assoc)),20)
```
Из этого делаем вывод, что большинство принимавших участие в опросе мужчины, большинство владеют python и советуют использовать его регулярно, используют matplotlib для визуализации данных, но иногда Seaborn, используют в качестве IDE jupiter notebook. Более половины используют линейную либо логистическую регрессию, вторым по популярности является деревья решений, либо же рандомные леса. Приличное число людей получили высшее образование. Треть владеет SQL. Scikit learn и TensorFlow являются самыми популярными фреймворками машинного обучения. Основными медиа ресурсами по дата саенс, которыми пользуются люди, являются Youtube и Kaggle, но Kaggle проводил опрос, так что он не считается.  
Посмотрим на самые редко выбираемые варианты
```{r}
head(sort(itemFrequency(data_assoc)),20)
```
Как видно из результатов, наделали множество инструментов business intelligence, а их почти никто не использует. Очень мало людей решило по приколу написать, что в первую очередь дата саентисту  нужно учить swift, либо bash. Зарплату выше 250k в год получают мало людей, меньше процента. Очень мало людей с гендером, иным, чем мужчина или женщина. Это можно обьяснить тем, что обычно люди рождаются либо мужского либо женского пола.    

Майним ассоциативные правила. Confidence 80% - хорошая уверенность. Так как у нас нету предпочтения, чтобы выведенные правила касались прям всего сообщества, ставим малый support.
```{r}
rules=apriori(data_assoc, parameter = list(support = 0.1, confidence = 0.8, target = "rules"))
```
Получаем количество созданных правил
```{r}
length(rules)
```
Строим график с support и confidence на осях, и lift в качестве shading.
```{r}
plot(rules)
```

Посмотрим правила, у которых lift>4
```{r}
subrules <-rules[rules@quality$lift > 4]
length(subrules)
inspect(subrules)
```
Слишком часто мы видим два слова "R" и "Rstudio" во всех правилах. Посмотрим на правые части  этих правил.
```{r}
inspect(subrules@rhs)
```
Как видно, использование языка R и IDE RStudio одновременно - очень частое явление, эти явления слишком сильно связаны, и поэтому повлияли на топ правил, полностью заполонив его: в правилах либо используется эти два явления и какое-нибудь популярное другое явление, например использование языка python, либо то, что действительно связано с этим явлениями - а именно использование ggplot/ggplot2. Уберем эти 2 item.
```{r}
data_assoc_len = 1:(length(data_for_assoc))
index_vector6 = data_assoc_len[names(data_for_assoc)[data_assoc_len]!="What.programming.languages.do.you.use.on.a.regular.basis...Select.all.that.apply....Selected.Choice...R" & names(data_for_assoc)[data_assoc_len]!="Which.of.the.following.integrated.development.environments..IDE.s..do.you.use.on.a.regular.basis....Select.all.that.apply....Selected.Choice....RStudio"]
data_for_assoc2 = data_for_assoc[index_vector6]
data_assoc2 = as(data_for_assoc2,"transactions")
```
Снова находим правила
```{r}
rules2=apriori(data_assoc2, parameter = list(support = 0.1, confidence = 0.8, target = "rules"))
length(rules2)
```
Строим график
```{r}
plot(rules2)
```

Находим правила с высоким lift
```{r}
subrules2 <-rules2[rules2@quality$lift > 3.6]
length(subrules2)
inspect(subrules2)
```
Чтобы было более ясно, посмотрим на самые часто встречающиеся в левой части items.
```{r}
tail(sort(itemFrequency(subrules2@lhs)),8)
```
Как видно, теперь в правила входят исключительно те items, которые встречаются очень часто в датасете, это стандартные параметры для среднего участника опроса.  
Чтобы увидеть зависимости с более редкими items, уменьшим confidence но увеличим lift. Чтобы увидеть эти более редкие itemsetы, уменьшим support
```{r}
rules3=apriori(data_assoc2, parameter = list(support = 0.05, confidence = 0.5, target = "rules"))
```
```{r}
plot(rules3)
```
```{r}
subrules3 <-rules3[rules3@quality$lift > 7]
length(subrules3)
inspect(subrules3)
```
Теперь появляется много правил, где очевидно есть и должна быть сильная зависимость между правой частью и одним из items в левой части, остальные items в левой части никак не связаны с левой частью. Чтобы преодолеть это, давайте ограничим длину правил до 2. Теперь можно вообще не ставить ограничений на support, алгоритм будет завершать работу, когда будет достигнута максимальная длина
```{r}
rules4=apriori(data_assoc, parameter = list(support = 0, confidence = 0.7, target = "rules",maxlen = 2))
```
Далее будет summary того, что найдено из правил
```{r}
subrules4 <-rules4[rules4@quality$lift > 7]
length(subrules4)
inspect(subrules4)
```
Выводы из этих правил: во первых, много правил, говорящих, что люди, которые используют какое нибудь средство, например какой нибудь business intelligence tool, используют его наиболее часто из всех средств, то есть обычно люди выбирают средство и на нем останавливаются, больше не используя никакое другое. Так же есть правила, устанавливающие связь между продуктами одной компании, если они используют один продукт компании, то скорее всего они используют другой, в смежной с ним области, то есть погружаются в экосистему этой компании, например люди, которые используют Google Cloud Vision AI для машинного обучения, так же скорее всего используют Google Cloud Platform GCP для облачных вычислений. Более всего в погружении людей в свою экосистему приуспевают Google и Amazon. Люди, которые используют Alteryx, скорее всего используют Tableau(это business intelligence tools). Люди, владеющие Contextualized.embeddings..ELMo..CoVe., скорее всего так же шарят за Transformer.language.models..GPT.3..BERT..XLnet.. , Encoder.decorder.models..seq2seq..vanilla.transformers и Word.embeddings.vectors..GLoVe..fastText..word2vec. Что касаемо фреймворков машинного обучения, то люди, использующие CatBoost так же используют LightGBM, между ними есть какая то связь!  
Посмотрим правила с чуть меньшим, но еще хорошим lift, вдруг там есть что то интересное
```{r}
subrules5 <-rules4[rules4@quality$lift <= 7 & rules4@quality$lift > 4]
length(subrules5)
inspect(subrules5)
```
Люди, которые используют очень редкое средство business intelligence "Domo" предпочитают облачные вычисления на амазоне. А пользователи Sisense скорее всего будут использовать MySQL и Github. Амазон так же очень нравится пользователям Einstein Analytics,Looker и облачной платформы Snowflake. Почему-то самым верным признаком того, что человек владеет github, является использование различных методов машинного обучения. Связи между языком R, его пакетами Shiny,Caret и ggplot, его фреймворком Tidymodels и средой RStudio теперь установлена официально. Чтобы шарить за рекуррентные нейронные сети, можно знать Contextualized.embeddings..ELMo..CoVe., и тут эта технология стала залогом знания в других областях. Использование CatBoost сопряжено с использованием Xgboost. Есть некоторые связи, которые возникли из-за методики проведения опроса, когда некоторые вопросы задаются, если был дан "актуальный" ответ на какой-то из других вопросов, особенно это относится к вопросам про NLP. такие правила мы исключаем из рассмотрения.  
Пора приблизиться к тому порогу, когда правила начинают возникать из-за случайностей. Хотелось бы увидеть чуть чуть зависимостей с более популярными ответами.  
```{r}
subrules6 <-rules4[rules4@quality$lift <= 4 & rules4@quality$lift > 3]
length(subrules6)
inspect(subrules6)
```
Языком R пользуются те, кто работает в роли статистика.Люди, которые зарабатывают более миллиона рублей в месяц, дают нам некоторые советы.Чтобы зарабатывать как они, нам придется пытаться применять методы машинного обучения в новых, неизведанных областях. Но при этом не использовать автоматические средства машинного обучения, такие как tpot, которые используются в этой работе.  
Строим Graph-based visualization, увеличивая лимит отображения.
```{r}
subrules7 <-rules4[rules4@quality$lift > 3]
plot(subrules7, method = "graph", engine = "htmlwidget", control = list(max=300))
```

Как мы видим, образовались несколько островков правил, сильно связанных между собой. Самый большой островок - это сервисы компании Amazon. Еще один островок  основан на облачных средствах google. Есть маленький островок вокруг Microsoft Azure. Островок вокруг GitHub никак нельзя классифицировать - просто очень много людей его используют. Еще островок вокруг языка R, на нем множество средств, связанных с ним. Есть так же целый кластер из людей, которые много на что ответили "None", вообщем они еще новички.
