---
title: "final_lab_clustering"
author: "andrey"
date: "08 01 2021"
output: html_document
---

## Clustering

Восстановим матрицу, которую мы делали для ассоциативных правил.
Подключаем нужные пакеты для ассоциативных правил
```{r, message=FALSE}
library('arules')
```
Подключаем пакет для использования регулярных выражений
```{r}
library(dplyr)
```
Загружаем данные, так, чтобы заголовками были названия вопросов
```{r}
data2 = read.csv("kaggle_survey_2020_responses.csv", skip = 1,na.strings="")
```
Убираем 1 столбец и supplementary questions
```{r}
data3 = data2[2:256]
```
Делаем новый датасет только из тех столбцов, в которых бинарные значения
```{r}
data_binary = is.na(select(data3, contains("Select.all.that.apply")))==FALSE
```
Делаем новый датасет из оставшихся столбцов
```{r}
data_non_binary = select(data3, !contains("Select.all.that.apply"))
```
Преобразуем датасет в факторы
```{r}
for (i in 1:length(data_non_binary))
{
  data_non_binary[[i]] = factor(data_non_binary[[i]])
}
```
Делаем один общий датасет из этих двух датасетов
```{r}
data_for_assoc = cbind(data_non_binary,data_binary)
```
Преобразуем это в обьект для использования ассоциативных правил
```{r}
data_assoc = as(data_for_assoc,"transactions")
```
Мы получили в data_assoc матрицу, где все бинарные переменные преобразованы в true/false, небинарные, но факторные, преобразованы в набор бинарных столбцов. Если заменить true/false на 1/0 , то данные будут полностью подходить для использования кластеризации, это получатся обьекты с кучей измерений, в каждом измерении есть только 2 значения: 0 и 1. 
Извлекаем из data_assoc матрицу особого типа, преобразуя в обычную матрицу и транспонируя, чтобы стала нормальная матрица, такая как нам нужно
```{r}
data_assoc@data@Dimnames[[1]] = data_assoc@itemInfo$labels
data_for_clustering = as.data.frame(t(as.matrix(data_assoc@data)))
```
Преобразуем true/false в 1/0
```{r}
for (i in 1:length(data_for_clustering))
{
  data_for_clustering[[i]] = as.integer(data_for_clustering[[i]])
}
```
Теперь данные готовы, чтобы использовать на них алгоритмы кластеризации
Определяем приемлемое значение для k
```{r}
round_num = 10
wss <-numeric(round_num)
for (i in 1:round_num) wss[i]<-kmeans(data_for_clustering, i, nstart = 1)$tot.withinss
plot(1:round_num, wss, type="b", xlab="Number of Clusters",ylab="Within groups sum of squares")
```

Даже с round_num=10 и 1 выбором рандомного набора время кластеризации 24 сек. Попробуем иерархическую кластеризацию. Делаем матрицу дистанций.
```{r}
dist_matrix = dist(data_for_clustering)
```
Выполнялось 14 минут, зато теперь мы можем проводить с ней кучу различных кластеризаций, которые будут выполняться быстро.
Подумаем, какой linkage нам нужен.  
single и complete - смотреть по наибольшему и наименьшему расстоянию, тут не нужно, у нас наверняка будут промежуточные точки между реально важными кластерами. Average - тоже не хочется, он соединяет кластеры с маленькими отклонениями, тут не будет гарантии такого маленького отклонения. Поэтому остановимся на методе с центроидами.  
Проводим кластеризацию
```{r}
hc_centroid<-hclust(dist_matrix, "centroid")
```
Выполняется всего за 20 сек.  
Построим график высот для различных значений k.Так как для анализа результатов вручную пригодны лишь количества кластеров до 100, берем только такие значения.
```{r}
plot(100:1, tail(hc_centroid$height,100), type="b", xlab="Number of Clusters",ylab="Height")
```

Локтем на графике является значение k=17. Берем такое k.
```{r}
cut_centroid = cutree(hc_centroid,k = 17)
plot(hc_centroid)
table(cut_centroid)
```

Как мы видим, с таким методом кластеризации образовался большой кластер, и далее к нему прилипали оставшиеся точки. Чтобы такого не было, попробуем другой метод кластеризации - взвешенный centroid linkage, то есть median
```{r}
hc_median<-hclust(dist_matrix, "median")
```
```{r}
plot(100:1, tail(hc_median$height,100), type="b", xlab="Number of Clusters",ylab="Height")
```

Локтем на графике является значение k=14. Берем такое k.
```{r}
cut_median = cutree(hc_median,k = 14)
plot(hc_median)
table(cut_median)
```

Тоже не подходит. Попробуем все методы кластеризации и попробуем найти тот, который даст хоть какое-то нормальное разбиение на кластеры.
```{r}
hc_single<-hclust(dist_matrix, "single")
cut_single = cutree(hc_single,k = 50)
table(cut_single)
```
```{r}
hc_complete<-hclust(dist_matrix, "complete")
cut_complete = cutree(hc_complete,k = 50)
table(cut_complete)
```
```{r}
hc_average<-hclust(dist_matrix, "average")
cut_average = cutree(hc_average,k = 50)
table(cut_average)
```
```{r}
hc_mcquitty<-hclust(dist_matrix, "mcquitty")
cut_mcquitty = cutree(hc_mcquitty,k = 50)
table(cut_mcquitty)
```
```{r}
hc_wardD<-hclust(dist_matrix, "ward.D")
cut_wardD = cutree(hc_wardD,k = 50)
table(cut_wardD)
```
```{r}
hc_wardD2<-hclust(dist_matrix, "ward.D2")
cut_wardD2 = cutree(hc_wardD2,k = 50)
table(cut_wardD2)
```
complete и mcquitty(взвешенный average) дали разделение на кластера, ward.D и ward.D2 дали очень равные по размеру кластера. Проанализируем каждое из этих разбиений. Начнем с complete.  
```{r}
plot(100:1, tail(hc_complete$height,100), type="b", xlab="Number of Clusters",ylab="Height")
```

Локоть k = 5.
```{r}
cut_complete = cutree(hc_complete,k = 5)
table(cut_complete)
plot(hc_complete)
```

В последнюю очередь присоединяются далекие от остальных отдельные точки, так не уследить за кластерами.
Взвешенный average
```{r}
plot(100:1, tail(hc_mcquitty$height,100), type="b", xlab="Number of Clusters",ylab="Height")
```

Локоть k = 10
```{r}
cut_mcquitty = cutree(hc_mcquitty,k = 10)
table(cut_mcquitty)
plot(hc_mcquitty)
```
Тут та же самая проблема.  
Ward.D
```{r}
plot(100:1, tail(hc_wardD$height,100), type="b", xlab="Number of Clusters",ylab="Height")
```
Локоть k = 18. Хотя много точек можно воспринимать как локти, забегая вперед скажу, что количество кластеров до 10 не сделает полноценного разделения, и лучше количество кластеров брать исходя из дендрограммы, когда видишь внизу нее маленькие сгустки.
```{r}
cut_wardD = cutree(hc_wardD,k = 18)
table(cut_wardD)
plot(hc_wardD)
```

Наконец-то нормальное разделение  
Ward.D2
```{r}
plot(100:1, tail(hc_wardD2$height,100), type="b", xlab="Number of Clusters",ylab="Height")
```
Локоть k = 7
```{r}
cut_wardD2 = cutree(hc_wardD2,k = 7)
table(cut_wardD2)
plot(hc_wardD2)
```

Посмотрим на получившиеся центроиды. Так как hclust не дает по умолчанию их координаты, придется их высчитывать вручную. summary_frame_wardD будет содержать координаты всех центроидов в измерениях, соответствующих вопросам. По сути, каждая такая координата представляет собой вероятность, что человек из данного кластера ответит на данный вопрос. 
```{r}
cut_wardD_factor = factor(cut_wardD)
summary_frame_wardD = head(data_for_clustering,length(levels(cut_wardD_factor)))
for (i in 1:length(summary_frame_wardD))
{
  summary_frame_wardD[[i]] = tapply(data_for_clustering[[i]],cut_wardD_factor,mean)
}
```
Скорее всего кластеры должны характеризоваться наличием каких то общих признаков, вероятность появления этих признаков должна быть больше, чем в среднем по датасету. Поэтому делим значения в колонках на средние значения для данной колонки в датасете, чтобы узнать, относительно всего датасета, во сколько раз чаще появляется этот признак у того или иного кластера
```{r}
factor_massiv = factor(integer(length(data_for_clustering[[1]])))
for (i in 1:length(summary_frame_wardD))
{
  summary_frame_wardD[[i]] = summary_frame_wardD[[i]]/as.numeric(tapply(data_for_clustering[[i]],factor_massiv,mean))
}
```
И теперь, чтобы увидеть самые характерные признаки для каждого кластера, выводим топ таких признаков. Чтобы функция tail сработала, транспонируем промежуточный результат сортировки. Итак, первый кластер
```{r}
tail(t(sort(summary_frame_wardD[1,])),20)
```
Чтобы понимать, что общее во многих кластерах, нужно быть экспертом в предметной области. Но тут мы видим, что первый кластер выделяют люди, которые используют нейронные сети и компьютерное зрение, используют соответствующие инструменты для этого, такие как PyTorch, TensorFlow,Keras. Среди свойств видны характерные для этого кластера методы, инструменты. При этом среди таких много молодых студентов, особенно из Азии, которые решили преисполниться в новой модной области.  
Кластер 2
```{r}
tail(t(sort(summary_frame_wardD[2,])),20)
```
Тут мы видим повышенное число людей, пользующихся облачными вычислениями и другими средствами крупных компаний Amazon, Google, Microsoft. Исходя из результатов исследования ассоциативных правил, можно предположить, что этот кластер в дальнейшем разделился бы на пользователей средств Amazon, пользователей Google и Microsoft Azure.  
Кластер 3
```{r}
tail(t(sort(summary_frame_wardD[3,])),20)
```
В этом кластере намного больше людей, которые на все ответили "None".Учитывая размер кластера почти в 2000, это не выбросы. Судя по всему эти люди совсем ничего не знают о Data Science.  
Кластер 4
```{r}
tail(t(sort(summary_frame_wardD[4,])),20)
```
Тут у нас люди, которые еще новички в data science. Это молодые студенты, они пока что изучают основные языки программирования, которые даже не так сильно относятся к data science. Причем тут много людей нетрадиционного гендера, это обьясняется современной культурой, в которой они варятся, и наличием в свойствах некоторых стран Юго-Восточной Азии, особенно Малайзии, в одной из этих стран считается, что там очень много таких небинарных людей(Таиланд или Филлипины, не помню в какой из них).  
Кластер 5
```{r}
tail(t(sort(summary_frame_wardD[5,])),20)
```
В этом кластере сгруппировались активные пользователи языка R. В свойствах перечислены куча пакетов R, таких как ggplot2,Shiny,RStudio,Caret,Tidymodels, а как же среда Rstudio. Причем, так как это R, эти люди статистики, и в их свойства попало так же использование продвинутых статистических средств, таких как SPSS, SAS. Некоторые страны, указанные в различных кластерах, для них нету обьяснения, как они туда попали: для них можно считать, что мы установили некое предпочтение у этих стран в плане тех методов, которые они любят использовать. Так, статистиков на R больше, чем в среднем, в Италии, Кении и Бельгии.  
Кластер 6
```{r}
tail(t(sort(summary_frame_wardD[6,])),20)
```
Этот кластер характеризуются студентами, которые пишут код и программируют не более 2 лет, используют популярные алгоритмы - регрессии и деревья с вариациями,они пользуются различными ноутбуками, например кагловским или от IBM, используют тривиальные места получения информации о data science - кагл и ютуб, могут ходить на курсы Udemy и пользоваться python. Вообщем начинающие в data science.  
Кластер 7
```{r}
tail(t(sort(summary_frame_wardD[7,])),20)
```
Про этих людей нельзя что то сказать конкретного, просто программисты, которые пользуются ноутбуками, но не из популярных. Про этот кластер нельзя точно что то сказать, возможно он возник случайно.  
Кластер 8
```{r}
tail(t(sort(summary_frame_wardD[8,])),20)
```
Тут что то связанное с bisuness intelligence, базами данных и системами хранения данных, в том числе облачными. Но рейтинги тут нормальные, то есть это не рандомно созданная группа.Среди средств есть известный Qlik. Эти люди предпочитают использовать эти средства при работе с данными, нежели языки программирования.  
Кластер 9
```{r}
tail(t(sort(summary_frame_wardD[9,])),50)
```
У этого кластера пока что рекордные коэффициенты. Тут люди, шарящие в машинном обучении и использующие автоматические средства машинного обучения. Кроме того, они очень активно используют облачные вычисления, должно быть для машинного обучения нужно много вычислений.  
Кластер 10
```{r}
tail(t(sort(summary_frame_wardD[10,])),20)
```
В эту группу попало много людей, которые называют свои зарплаты. Должно быть это группа работающих людей, которые обычно software engineer либо data analyst  
Кластер 11
```{r}
tail(t(sort(summary_frame_wardD[11,])),20)
```
В этой группе молодые люди, не получившие должного образования в сфере IT, но лезущие туда. Они вероятно знают некоторые популярные языки программирования, такие как C++ или Java, выучили их, потому что они популярны. В Data science они особо не разбираются, и возможно даже не знают что значит слово IDE. И тем не менее они знают, что такое bash. Непонятно, как это все связано, возможно они пользователи linux, знают bash и умеют компилировать код без ide.  
Кластер 12
```{r}
tail(t(sort(summary_frame_wardD[12,])),30)
```
Опять что то связанное с базами данных и системами хранения данных. Причем эти люди работают и получают зарплату.
Кластер 13
```{r}
tail(t(sort(summary_frame_wardD[13,])),20)
```
Эта группа скорее всего относится к нескольким связанным вопросам, когда некоторые из них задавались, когда были "актуальные" ответы на предыдущие. Это вопросы 17-19.
Кластер 14
```{r}
tail(t(sort(summary_frame_wardD[14,])),20)
```
Это группа тоже связана с этими вопросами, а так же вопросами 33a-34a.  
Кластер 15
```{r}
tail(t(sort(summary_frame_wardD[15,])),20)
```
Еще одна группа, относящаяся к спец вопросам.
Кластер 16
```{r}
tail(t(sort(summary_frame_wardD[16,])),20)
```
Эта группа из бизнес-аналитиков и менеджеров, они никогда не писали код, в data science не понимают.
Кластер 17
```{r}
tail(t(sort(summary_frame_wardD[17,])),20)
```
Еще одна группа людей, которые ничего не знают в data science, но некоторые языки программирования знают, такие как Java,Javascript,MATLAB,Swift.  
Кластер 18
```{r}
tail(t(sort(summary_frame_wardD[18,])),30)
```
В эту группу попало много азиатов, а так группа с низкими коэффициентами и ничего особо не характеризуется. Причем получается, большинство коэффициентов ниже 0, нет даже много "None". Скорее всего этим людям наскучил опрос очень быстро, и дальше вопроса про страну они не продвинулись и закрыли вкладку.
Я думаю, один такой метод кластеризации дал достаточно информации о различных группах, принимавших участие в опросе.
